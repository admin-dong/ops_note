:<<BOGE

fs.file-max = 1000000

这个参数定义了系统中最大的文件句柄数。文件句柄是用于访问文件的数据结构。增加这个值可以提高系统同时打开文件的能力。

fs.inotify.max_user_instances = 8192

inotify是Linux内核中的一个机制，用于监视文件系统事件。这个参数定义了每个用户可以创建的inotify实例的最大数量。

net.ipv4.tcp_syncookies = 1

当系统遭受SYN洪水攻击时，启用syncookies可以防止系统资源被耗尽。SYN cookies是一种机制，用于在TCP三次握手中保护服务器端资源。

net.ipv4.tcp_fin_timeout = 30

这个参数定义了TCP连接中，等待关闭的时间。当一端发送FIN信号后，等待对端关闭连接的超时时间。

net.ipv4.tcp_tw_reuse = 1

启用该参数后，可以允许将TIME-WAIT状态的TCP连接重新用于新的连接。这可以减少系统中TIME-WAIT连接的数量。

net.ipv4.ip_local_port_range = 1024 65000

这个参数定义了本地端口的范围，用于分配给发送请求的应用程序。它限制了可用于客户端连接的本地端口范围。

net.ipv4.tcp_max_syn_backlog = 16384

这个参数定义了TCP连接请求的队列长度。当系统处理不及时时，超过该队列长度的连接请求将被拒绝。

net.ipv4.tcp_max_tw_buckets = 6000

这个参数定义了系统同时保持TIME-WAIT状态的最大数量。超过这个数量的连接将被立即关闭。

net.ipv4.route.gc_timeout = 100

这个参数定义了内核路由表清理的时间间隔，单位是秒。它影响路由缓存的生命周期。

net.ipv4.tcp_syn_retries = 1

这个参数定义了在发送SYN请求后，等待对端回应的次数。超过指定次数后仍未响应，连接将被认为失败。

net.ipv4.tcp_synack_retries = 1

这个参数定义了在发送SYN+ACK回应后，等待对端发送ACK的次数。超过指定次数后仍未收到ACK，连接将被认为失败。

net.core.somaxconn = 32768

这个参数定义了监听队列的最大长度。当服务器正在处理的连接数超过此值时，新的连接请求将被拒绝。

net.core.netdev_max_backlog = 32768

这个参数定义了网络设备接收队列的最大长度。当接收队列已满时，新的数据包将被丢弃。

net.core.netdev_budget = 5000

这个参数定义了每个网络设备接收队列在每个时间间隔中可以处理的数据包数量。

net.ipv4.tcp_timestamps = 0

禁用TCP时间戳。时间戳可以用于解决网络中的数据包乱序问题，但在高负载环境下可能会增加开销。

net.ipv4.tcp_max_orphans = 32768

这个参数定义了系统中允许存在的最大孤立（没有关联的父连接）TCP连接数量。超过这个数量的孤立连接将被立即关闭。

BOGE

[ -z "$(grep 'fs.file-max' /etc/sysctl.conf)" ] && cat >> /etc/sysctl.conf << EOF

fs.file-max = 1000000

fs.inotify.max_user_instances = 8192

net.ipv4.tcp_syncookies = 1

net.ipv4.tcp_fin_timeout = 30

net.ipv4.tcp_tw_reuse = 1

net.ipv4.ip_local_port_range = 1024 65000

net.ipv4.tcp_max_syn_backlog = 16384

net.ipv4.tcp_max_tw_buckets = 6000

net.ipv4.route.gc_timeout = 100

net.ipv4.tcp_syn_retries = 1

net.ipv4.tcp_synack_retries = 1

net.core.somaxconn = 32768

net.core.netdev_max_backlog = 32768

net.core.netdev_budget = 5000

net.ipv4.tcp_timestamps = 0

net.ipv4.tcp_max_orphans = 32768

EOF

sysctl -p

————————————————

Linux 内核优化可以分为以下几个类别，针对不同方面进行调整和优化

1. 内存管理优化：涉及内存分配、页面调度、缓存管理等方面的优化。这包括调整内存分配策略、页面置换算法、页表管理方式等，以提高内存使用效率和性能。

- - /proc/sys/vm/：包含与虚拟内存管理相关的参数，如页面大小、页面调度算法等。
  - /proc/sys/kernel/：包含与内核行为和内存管理相关的参数，如OOM（Out of Memory）行为、内存回收策略等。

1. 调度器优化：涉及进程调度和 CPU 资源分配的优化。调度器负责决定哪个进程在给定时间片内运行，以及如何分配 CPU 时间。调整调度策略和参数可以改善系统的响应性能和负载均衡。

- - /proc/sys/kernel/sched/：包含与调度器相关的参数，如调度策略、时间片长度等。

1. 文件系统优化：涉及文件系统的性能和可靠性方面的优化。包括选择合适的文件系统类型、调整文件系统参数、启用适当的文件系统特性等，以提高文件系统的读写性能和数据保护能力。

- - /proc/sys/fs/：包含与文件系统相关的参数，如缓存管理、文件句柄限制等。

1. 网络优化：涉及网络栈和网络设备的优化。包括调整网络协议栈参数、网络缓冲区大小、队列管理算法等，以提升网络传输性能和吞吐量。

- - /proc/sys/net/：包含与网络协议栈和网络设备相关的参数，如网络缓冲区大小、连接追踪设置等。

1. I/O 子系统优化：涉及磁盘 I/O、块设备和存储系统的优化。这包括调整磁盘调度算法、启用磁盘缓存、优化 RAID 设置等，以提高磁盘访问性能和数据可靠性。

- - /proc/sys/block/：包含与块设备和磁盘 I/O 相关的参数，如磁盘调度算法、读写缓存设置等。

1. 安全性和稳定性优化：涉及内核安全性和稳定性方面的优化。包括启用适当的安全功能、设置合理的权限和访问控制、修复漏洞和错误等，以提高系统的安全性和稳定性。

- - /proc/sys/kernel/：包含与内核安全性和稳定性相关的参数，如系统调用限制、地址空间随机化等。

## 内核优化参考示例

```
net.ipv4.ip_forward = 1 #是否开启数据包转发
net.ipv4.tcp_keepalive_time = 600	 # 表示 TCP 连接的空闲时间阈值，默认值为 7200 秒，当一个 TCP 连接处于空闲状态（没有数据传输）并超过这个阈值时，系统会开始发送保活探测数据包
net.ipv4.tcp_keepalive_probes = 10   # 默认值为 9，表示在认定连接失效之前发送的保活探测数据包的数量。当一个 TCP 连接处于空闲状态并超过空闲时间阈值时，系统会开始发送保活探测数据包
net.ipv4.tcp_keepalive_intvl = 30    # 默认值为 75 秒，表示连续发送保活探测数据包之间的时间间隔。
net.ipv4.tcp_timestamps = 0			 # 禁用 TCP 时间戳选项，从而减少数据包的大小和处理开销。默认值为 1，表示启用 TCP 时间戳选项。
net.ipv4.tcp_orphan_retries = 3		 # 表示孤立连接的重试次数，孤立连接是指服务器端已经关闭了连接，但客户端仍然保持连接打开状态，当服务器端检测到孤立连接时，会发送 FIN 包来终止连接。该参数指定了服务器在终止孤立连接之前的重试次数。
net.ipv4.tcp_syncookies = 1          # 启用SYN cookies，减少半开连接数量
net.ipv4.tcp_tw_reuse = 1            # 启用TIME-WAIT重用
net.ipv4.tcp_fin_timeout = 1		 #默认值为 60 秒，指定了 TCP 连接处于 FIN-WAIT-2 状态的超时时间
net.ipv4.tcp_max_syn_backlog = 16384 #指定了 SYN 队列的最大长度，用于处理传入连接请求
net.ipv4.tcp_max_tw_buckets = 131072 #表示系统同时保持TIME_WAIT的最大数量，如果超过这个数字，TIME_WAIT将立刻被清除并打印警告信息。
net.ipv4.tcp_rmem = 4096	87380	6291456 #TCP 套接字接收缓冲区的最小值、默认值和最大值（以字节为单位）。
net.ipv4.tcp_wmem = 4096    20480   4194304 #TCP 套接字发送缓冲区的最小值、默认值和最大值（以字节为单位）。
net.ipv4.tcp_mem = 4096 8192 16777216  #TCP 套接字共享的内存池的最小值、默认值和最大值（以页为单位）。
net.nf_conntrack_max = 25000000
net.netfilter.nf_conntrack_max = 25000000 #指定了连接跟踪表的最大条目数。连接跟踪表用于存储当前活动的网络连接信息。net.nf_conntrack_max 是旧的参数名，net.netfilter.nf_conntrack_max 是新的参数名。两者具有相同的功能。较大的连接跟踪表大小可以容纳更多的活动连接，适用于高负载的网络环境。
net.netfilter.nf_conntrack_tcp_timeout_established=180 #定了 TCP 连接在状态为 "ESTABLISHED"（已建立连接）时的超时时间（以秒为单位）。内核会根据超时时间来判断连接是否超时并将其从连接跟踪表中删除。
net.netfilter.nf_conntrack_tcp_timeout_time_wait=120 #指定了 TCP 连接在状态为 "TIME_WAIT"（等待关闭）时的超时时间（以秒为单位）。内核会根据超时时间来判断连接是否超时并将其从连接跟踪表中删除。
net.netfilter.nf_conntrack_tcp_timeout_close_wait=60 # TCP 连接在状态为 "CLOSE_WAIT"（等待关闭确认）时的超时时间（以秒为单位）。内核会根据超时时间来判断连接是否超时并将其从连接跟踪表中删除。
net.netfilter.nf_conntrack_tcp_timeout_fin_wait=12 #TCP 连接在状态为 "FIN_WAIT"（等待远端关闭）时的超时时间（以秒为单位）。内核会根据超时时间来判断连接是否超时并将其从连接跟踪表中删除。
net.core.somaxconn =  326780		# 增加监听队列长度 系统中每个监听套接字（如 TCP 或 UDP）的最大挂起连接队列长度
vm.swappiness = 0					 # 关闭swap分区，降低系统对swap的使用，定义了系统对swap的使用倾向
kernel.pid_max=4194303				 # 系统可以分配的最大进程标识符（PID）数量
fs.inotify.max_user_watches=1048576  #Inotify 是 Linux 内核提供的一种机制，用于监视文件系统中的文件和目录的变化。通过增加 max_user_watches 参数的值，可以增加每个用户可以同时监视的文件和目录的数量。
fs.inotify.max_user_instances=8192 # 可以增加每个用户可以同时创建的 Inotify 实例的数量。
fs.inotify.max_queued_events=16384 # 可以增加每个 Inotify 实例可以排队的事件的数量。
kernel.sysrq = 1						 # 启用SysRq可以在系统出现问题时提供一种安全的操作方式。
kernel.softlockup_all_cpu_backtrace = 1 #启用或禁用内核在检测到软锁定（soft lockup）时记录所有 CPU 的回溯信息,软锁定指的是内核中某个 CPU 被长时间占用而无法响应其他任务的情况。当该参数设置为 1 时，内核会记录所有 CPU 的回溯信息，以便诊断软锁定问题
kernel.softlockup_panic=1 #当该参数设置为 1 时，内核会在检测到软锁定时触发系统崩溃，以确保系统可以尽快恢复正常运行。触发系统崩溃可以帮助诊断软锁定的原因，并采取相应的修复措施。

```

## 日志报错 fork：Cannot allocate memory （-bash: fork: 无法分配内存）处理

系统内部的总进程数超过了系统默认得进程数

```
echo "kernel.pid_max=1000000 " >> /etc/sysctl.conf;sysctl -p
```

## kernel: TCP: time wait bucket table overflow 错误处理

处于 time wait 状态的 TCP 套接字，超过 net.ipv4.tcp_max_tw_buckets 限制。

```
vim /etc/sysctl.conf
net.ipv4.tcp_max_tw_buckets= 16000 #将该值增大
```

## request_sock_tcp possible syn flooding on port错误处理 和WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 错误处理

增加 TCP 连接队列的最大长度（tcp_max_syn_backlog）：

- net.ipv4.tcp_max_syn_backlog 这个参数定义了 TCP 连接队列的最大长度。在高并发情况下，可以适当增大这个值，以防止连接队列溢出。

```
vim /etc/sysctl.conf
net.core.somaxconn = 2048
net.ipv4.tcp_max_syn_backlog = 2048
```

## TCP: out of memory -- consider tuning tcp_mem 错误处理

调整 TCP 接收缓冲区大小（tcp_rmem）和发送缓冲区大小（tcp_wmem）：

- net.ipv4.tcp_rmem = min default max
- net.ipv4.tcp_wmem = min default max 这些参数定义了 TCP 接收和发送缓冲区的最小、默认和最大大小。您可以适当增大这些值，以满足高并发场景的需求。

```
vim /etc/sysctl.conf
net.ipv4.tcp_rmem = 4096	87380	6291456
net.ipv4.tcp_wmem = 4096    20480   4194304
net.ipv4.tcp_mem=8388608 8388608 8388608
```

## inotify watch 耗尽

每个 linux 进程可以持有多个 fd，每个 inotify 类型的 fd 可以 watch 多个目录，每个用户下所有进程 inotify 类型的 fd 可以 watch 的总目录数有个最大限制，这个限制可以通过内核参数配置: fs.inotify.max_user_watches

查看最大 inotify watch 数:

```
$ cat /proc/sys/fs/inotify/max_user_watches
```

```
vim /etc/sysctl.conf
fs.file-max=1100000
```

## Too many open files 错误

三个参数：fs.nr_open、nofile（分为soft nofile 和 hard nofile ） 和 fs.file-max。

其实在 Linux 上能打开多少个文件，限制有两种：

- 第一种，进程级别的，限制的是单个进程上可打开的文件数。具体参数是 soft nofile 和 fs.nr_open。它们两个的区别是 soft nofile 可以不同用户配置不同的值。而 fs.nr_open 在一台 Linux 上只能配一次。
- 第二种，系统级别的，整个系统上可打开的最大文件数，具体参数是fs.file-max。但是这个参数不限制 root 用户。

另外这几个参数之间还有耦合关系，因此还要注意以下三点：

- 1、如果你想加大 soft nofile, 那么 hard nofile 也需要一起调整。因为如果 hard nofile 设置的低， 你的 soft nofile 设置的再高都没用，实际生效的值会按二者里最低的来。
- 2、如果你加大了 hard nofile，那么 fs.nr_open 也都需要跟着一起调整。**如果不小心把 hard nofile 设置的比 fs.nr_open 大了，后果比较严重。会导致该用户无法登陆。如果设置的是 \* 的话，那么所有的用户都无法登陆。**
- 3、还要注意如果你加大了 fs.nr_open，但是用的是 echo "xx" > /proc/sys/fs/nr_open 的方式，刚改完你可能觉得没问题。只要机器一重启你的 fs.nr_open 设置就会失效，还是会无法登陆。

```
# vi /etc/sysctl.conf
fs.nr_open=1100000  #要比 hard nofile 大一点
fs.file-max=1100000 #多留点buffer
# sysctl -p
# vi /etc/security/limits.conf
*  soft  nofile  1000000
*  hard  nofile  1000000
```

将文件限制设置为无限制不是好主意

## /etc/security/limits.conf 文件解析

/etc/security/limits.conf 文件用于设置Linux系统中用户或进程的资源限制。它定义了用户或用户组可以使用的系统资源的上限，包括进程数量、文件打开数、内存限制等。通过修改该文件，可以对系统用户和进程进行限制和优化

常见的值及其说明：

- soft：软限制，表示当前生效的资源限制值。软限制是可修改的，但不能超过硬限制。
- hard：硬限制，表示软限制的上限。只有超级用户（root）可以增加硬限制。硬限制是系统的实际限制。
- unlimited：表示无限制。将资源限制设置为无限制可以允许用户或进程使用系统的最大资源量。
- 数值：具体的资源限制值。例如，65535表示设置资源限制为 65535。

具体对用户和进程的限制项：#常用值并不是全部

- **nofile（文件打开数限制）**：这个参数用于限制用户或进程可以打开的文件描述符数量。
- **nproc（进程数量限制）**：这个参数用于限制用户或进程可以创建的最大进程数量。
- **stack（线程栈大小限制）**：这个参数用于限制用户或进程的线程栈大小。
- **memlock（锁定内存限制）**：这个参数用于限制用户或进程可以锁定的内存大小。

```
#示例
* soft nofile 655360
* hard nofile 131072
* soft nproc 655350
* hard nproc 655350
* soft memlock unlimited
* hard memlock unlimited
* soft stack   unlimited
* hard stack   unlimited
```

tcp内核调优

● net.ipv4.tcp_syn_retries: 表示了在一次主动打开申请中尝试重新发送SYN报文段的最大次数。 ubuntu 18.04中值为6

● net.ipv4.tcp_synack_retries: 表示在响应对方的一个主动打开请求时尝试重新发送SYN+ACK报文段的最大次数。默认为重试5次(约180秒)。

● net.ipv4.tcp_fin_timeout: TIME_WAIT状态也称为2MSL等待状态。在该状态中，TCP将会等待两倍于最大段生存期的时间，也被称为加倍等待。net.ipv4.tcp_fin_timeout的数值记录了2MSL状态需要等待的超时时间（以秒为单位）。该值取值范围为30~300秒。ubuntu 18.04中值为60; 规则是执行主动关闭操作的一端进入Time_wait状态并维持两倍的最大生存时间.这样有助于防止TCO处理同一条连接中旧实例的报文段.

● net.ipv4.tcp_tw_reuse此参数允许重新使用处于TIME-WAIT状态的套接字用于新的TCP连接.缓解服务器出现大量TIME-WAIT状态的连接导致端口资源耗尽. 开启此功能值为1,使得在安全的条件下重用TIME-WAIT连接.开启此功能可能会与某些负载均衡器的工作方式发生冲突.

● net.ipv4.tcp_max_syn_backlogcentos7下为128，当一个连接请求到达（即，SYN报文段），将会检查系统范围的tcp_max_syn_backlog。如果处于SYN_RECV状态的连接数目超过了这一阈值，进入的连接将会被拒绝。

● net.core.somaxconn每一个处于侦听状态下的节点都拥有一个固定长度的连接队列。其中的连接已经被TCP完全接收（即三次握手完成），但未被应用程序接收。应用程序会对这一队列做出限制。通常称为未完成连接（backlog）。backlog的数目必须在0与一个系统指定的最大值之间。该值称为net.core.somaxconn，默认值为128.

● net.ipv4.tcp_syncookiesSYN泛洪是一种TCP拒绝服务攻击，SYN cookies的主要思想是，当一个SYN到达时，这条链接存储的大部分信息都会被编码并保存在SYN+ACK报文段的序列号字段。采用SYN cookies的目标主机不需要为进入的连接请求分配任何存储资源——只有当SYN+ACK报文段本身被确认后（并且已返回初始序列号）才会分配真正的内存。连接也能够被设置为ESTABLISHED状态.

● net.ipv4.tcp_keepalive_intvl这个参数定义了两个连续的keepalive探测包之间的间隔时间,单位是秒.只有当TCP keepalive选项被启用时,设置才有效. net.ipv4.tcp_keepalive_intvl = 20

● net.ipv4.tcp_keepalive_probes参数设置了在TCP认为连接失效之前,未获得客户端确认的情况下将发送多少个keepavlie探测包.默认值9.如果在指定的尝试次数后仍未获得响应,系统则认定连接已经失效.   net.ipv4.tcp_keepalive_probes = 5

# 根据TCP/IP进行Linux内核参数优化

例1：调整访问服务端的【客户端】的动态端口范围 ，LVS（10-50万并发），NGINX负载，SQUID缓存服务,

[root@oldboy ~]# cat /proc/sys/net/ipv4/ip_local_port_range 

32768	60999

[root@oldboy ~]# echo 4000 65000 > /proc/sys/net/ipv4/ip_local_port_range 

[root@oldboy ~]# 

[root@oldboy ~]# cat /proc/sys/net/ipv4/ip_local_port_range 

4000	65000

# DOS/SYN攻击

<https://baike.baidu.com/item/dos%E6%94%BB%E5%87%BB/3792374?fr=aladdin>

*****企业案例：DOS攻击的案例：

SYN洪水攻击属于DoS攻击的一种，它利用TCP协议缺陷，通过发送大量的半连接请求，耗费CPU和内存资源。

例2：#全/半连接队列

/proc/sys/net/ipv4/tcp_max_syn_backlog  #syn连接池队列大小

[root@oldboy ~]# echo 8192 >/proc/sys/net/ipv4/tcp_max_syn_backlog

[root@oldboy ~]# cat /proc/sys/net/ipv4/tcp_max_syn_backlog

8192

/proc/sys/net/core/somaxconn #完成连接队列大小，默认值128,建议调整大小为1024以上

[root@oldboy ~]# echo 512 >/proc/sys/net/core/somaxconn

[root@oldboy ~]# cat /proc/sys/net/core/somaxconn

512

# 

# 超时连接

\# Linux为了防止孤儿连接长时间存留在内核中的两个内核参数： 

[root@oldboy ~]# cat /proc/sys/net/ipv4/tcp_max_orphans 

16384

[root@oldboy ~]# cat /proc/sys/net/ipv4/tcp_fin_timeout 

60

[root@oldboy ~]# echo 2 > /proc/sys/net/ipv4/tcp_fin_timeout 

[root@oldboy ~]# cat /proc/sys/net/ipv4/tcp_fin_timeout 

2

# #TCP超时重传 

 异常网络状况下（开始出现超时或丢包），TCP控制数据传输以保证其承诺的可靠服务TCP服务必须能够重传超时时间内未收到确认的TCP报文段。为此，TCP模块为每个TCP报文段都维护一个重传定时器，该定时器在TCP报文段第一次被发送时启动。如果超时时间内未收到接收方的应答，TCP模块将重传TCP报文段并重置定时器。至于下次重传的超时时间如何选择，以及最多执行多少次重传，就是TCP的重传策略 

与TCP超时重传相关的两个内核参数：

\#指定在底层IP接管之前TCP最少执行的重传次数，默认值是3

\#cat /proc/sys/net/ipv4/tcp_retries1

3

\#指定连接放弃前TCP最多可以执行的重传次数，默认值15（一般对应13～30min）

\#cat /proc/sys/net/ipv4/tcp_retries2 

15

# #####拥塞控制 

 网络中的带宽、交换结点中的缓存和处理机等，都是网络的资源。在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可承受的能力，网络的性能就会变坏。此情况称为拥塞TCP为提高网络利用率，降低丢包率，并保证网络资源对每条数据流的公平性。即所谓的拥塞控制TCP拥塞控制的标准文档是RFC 5681，

拥塞控制的四个部分：

\- 慢启动（slow start）

\- 拥塞避免（congestion avoidance）

\- 快速重传（fast retransmit）

\- 快速恢复（fast recovery）。

拥塞控制算法在Linux下有多种实现，比如reno算法、vegas算法和cubic算法等。

当前所使用的拥塞控制算法 

/proc/sys/net/ipv4/tcp_congestion_control

# #####内核TCP参数优化 

**编辑文件/etc/sysctl.conf，加入以下内容：然后执行sysctl -p让参数生效**。 

net.ipv4.tcp_fin_timeout = 2

net.ipv4.tcp_max_orphans = 16384

net.ipv4.ip_local_port_range = 2000 65000

net.ipv4.tcp_max_syn_backlog = 16384

net.core.somaxconn = 16384

net.ipv4.tcp_max_tw_buckets = 36000

net.ipv4.tcp_syn_retries = 1

net.ipv4.tcp_synack_retries = 1

net.ipv4.tcp_tw_reuse = 1

net.ipv4.tcp_tw_recycle = 1

net.ipv4.tcp_syncookies = 1

net.ipv4.tcp_keepalive_time = 600

net.ipv4.route.gc_timeout = 100

net.core.netdev_max_backlog = 16384

## 作用说明：

\- net.ipv4.tcp_tw_reuse 表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认值为0，表示关闭。

该参数对应系统路径为：/proc/sys/net/ipv4/tcp_tw_reuse 0 

\- net.ipv4.tcp_tw_recycle 表示开启TCP连接中TIME-WAIT sockets的快速回收。该参数对应系统路径为：/proc/sys/net/ipv4/tcp_tw_recycle，默认为0，表示关闭。

提示：reuse和recycle这两个参数是为防止生产环境下Web、Squid等业务服务器time_wait网络状态数量过多设置的 

\- net.ipv4.tcp_syncookies 表示开启SYN Cookies功能。当出现SYN等待队列溢出时，启用Cookies来处理，可防范少量SYN攻击。

该参数对应系统路径为：/proc/sys/net/ipv4/tcp_syncookies,默认值为1 

\- net.ipv4.ip_local_port_range 该选项用来设定允许系统打开的端口范围，即用于向外连接的端口范围。

该参数对应系统路径为：/proc/sys/net/ipv4/ip_local_port_range 32768 61000 

\- net.ipv4.tcp_max_syn_backlog 表示SYN队列的长度，即半连接队列长度，默认为1024。该参数为服务器端用于记录那些尚未收到客户端确认信息的连接请求最大值。该参数对象系统路径为：/proc/sys/net/ipv4/tcp_max_syn_backlog 

\- net.ipv4.tcp_max_tw_buckets 表示系统同时保持TIME_WAIT套接字的最大数量，如果超过这个数值，TIME_WAIT套接字将立刻被清除并打印警告信息。默认为180000，对于Nginx等服务器来说可以将其调低一点，如改为5000~30000，不通业务的服务器也可以给大一点，比如LVS、Squid。此项参数可以控制TIME_WAIT套接字的最大数量，避免Squid服务器被大量的TIME_WAIT套接字拖死。该参数对应系统路径为：/proc/sys/net/ipv4/tcp_max_tw_buckets 

*****企业案例：TIME_WAIT过多故障，如何解决？

LVS、Squid、Nginx这些软件本身服务，能够承受大并发。

基础解决：

net.ipv4.tcp_fin_timeout = 2

net.ipv4.tcp_max_tw_buckets = 36000

net.ipv4.tcp_tw_reuse = 1

net.ipv4.tcp_tw_recycle = 1

\- net.ipv4.tcp_synack_retries 参数的值决定了内核放弃连接之前发送SYN+ACK包的数量。该参数对应系统路径为：/proc/sys/net/ipv4/tcp_synack_retries,默认值为5 

\- net.ipv4.tcp_syn_retries 表示在内核放弃建立连接之前发送SYN包的数量。该参数对应系统路径为：/proc/sys/net/ipv4/tcp_syn_retries,默认值为6 

\- net.ipv4.tcp_max_orphans 用于设定系统中最多有多少个TCP套接字不被关联到任何一个用户文件句柄上。如果超过这个数值，孤立连接将被立即被复位并打印出警告信息。这个限制只有为了防止简单的DoS攻击。不能过分依靠这个限制甚至认为减少这个值，更多的情况是增加这个值。该参数对应系统路径为：/proc/sys/net/ipv4/tcp_max_orphans ，默认值8192 

\- net.core.somaxconn 同时发起的TCP的最大连接数，即全连接队列长度，在高并发请求中，可能会导致链接超时或重传，一般结合并发请求数来调大此值。该参数对应系统路径为：/proc/sys/net/core/somaxconn ，默认值是128 

\- net.core.netdev_max_backlog 表示当每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许发送到队列的数据包最大数。该参数对应系统路径为：/proc/sys/net/core/netdev_max_backlog，默认值为1000 

修改配置文件：/etc/sysctl.conf (原内容保存)

vm.swappiness = 10

\##swappiness的值的大小对如何使用swap分区是有着很大的联系的。swappiness=0的时候表示最大限度使用物理内存，然后才是 swap空间，swappiness＝100的时候表示积极的使用swap分区，并且把内存上的数据及时的搬运到swap空间里面。两个极端，对于 centos linux 5的默认设置，这个值等于60，建议修改为10。

net.ipv4.tcp_syncookies = 1

\##表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭；

net.ipv4.tcp_tw_reuse = 1

\#表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；

net.ipv4.tcp_tw_recycle = 1

\#表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。

net.ipv4.tcp_fin_timeout = 30

\#表示如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间。

net.ipv4.tcp_keepalive_time = 1200

\#表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为20分钟。

net.ipv4.ip_local_port_range = 1024 65000

\##表示用于向外连接的端口范围。缺省情况下很小：32768到61000，改为1024到65000。

net.ipv4.tcp_max_tw_buckets = 5000

\##表示系统同时保持TIME_WAIT套接字的最大数量，如果超过这个数字，

\##TIME_WAIT套接字将立刻被清除并打印警告信息。默认为180000，改为5000。

\##对于Apache、Nginx等服务器，上几行的参数可以很好地减少TIME_WAIT套接字数量，

\##但是对于Squid，效果却不大。此项参数可以控制TIME_WAIT套接字的最大数量，避免Squid服务器被大量的TIME_WAIT套接字拖死。

net.ipv4.tcp_mem = 786432 1048576 1572864

\##有3个值,意思是:

\##net.ipv4.tcp_mem[0]:低于此值,TCP没有内存压力.

\##net.ipv4.tcp_mem[1]:在此值下,进入内存压力阶段.

\##net.ipv4.tcp_mem[2]:高于此值,TCP拒绝分配socket

net.core.wmem_max = 873200

\##最大socket写buffer,可参考的优化值:873200

net.core.rmem_max = 873200

\##最大socket读buffer,可参考的优化值:873200

net.ipv4.tcp_wmem = 8192 436600 873200

\##TCP写buffer,可参考的优化值: 8192 436600 873200

net.ipv4.tcp_rmem = 32768 436600 873200

\##TCP读buffer,可参考的优化值: 32768 436600 873200

net.core.somaxconn = 256

\##listen()的默认参数,挂起请求的最大数量.默认是128.对繁忙的服务器,增加该值有助于网络性能.

可调整到256.

net.core.netdev_max_backlog = 1000

\##进入包的最大设备队列.默认是300,对重负载服务器而言,该值太低,可调整到1000.

net.ipv4.tcp_max_syn_backlog = 2048

\##进入SYN包的最大请求队列.默认1024.对重负载服务器,增加该值显然有好处.

可调整到2048.

net.ipv4.tcp_retries2 = 5

\##TCP失败重传次数,默认值15,意味着重传15次才彻底放弃.可减少到5,以尽早释放内核资源.

net.ipv4.tcp_keepalive_intvl = 30

net.ipv4.tcp_keepalive_probes = 3

\##意思是如果某个TCP连接在idle 30个分钟后,内核才发起probe.如果probe 3次(每次75秒)不成功,内核才彻底放弃,认为该连接已失效.

net.ipv4.conf.lo.arp_ignore = 0

net.ipv4.conf.lo.arp_announce = 0

net.ipv4.conf.all.arp_ignore = 0

net.ipv4.conf.all.arp_announce = 0

\##NAT模式下的打开ARP广播响应

# 